### Self Notes###
Classification: Used to predict categories
-> Contains both linear and non linear models

-> Logistic regression curve is called as the sigmoid curve and it is a linear model.
-> KNN assigns a new point based on how many neighbors of each cateogory it has. The new data point gets assigned to the category which has the most closest points.

We don't fit the X_test with the standard scaler as it causes data leakage.
i.e, if we fit the the Standard scaler with the X_test, the model already knows the values inside the test set before the prediction. This might make the model have more accuracy but it will cause the model to not perform the same with new data.
(It's like giving away questions way before an exam)

Used a very simple data set to learn new models.
The graphs are from chatGPT!

-> Logistic theorem does'nt necessarily require the features to be independent (I've mistaken about that). It just *assumes* that the features are independent

### Reading a Heatmap ###
+1: Perfect positive correlation
0: No correlation
-1: Perfect negative correlation

If two features are very similar (e.g., correlation > 0.8), logistic regression might:
Still work
But become unstable, especially if you interpret the model (coefficients become unreliable)
You don’t have to remove them always, but if you're cleaning data or building interpretable models, it’s good to be aware.
